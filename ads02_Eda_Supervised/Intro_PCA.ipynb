{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulated_variance(df, scale):\n",
    "    \"\"\"Apply PCA on a DataFrame and return a new DataFrame containing\n",
    "    the cumulated explained variance from with only the first component,\n",
    "    up to using all components together. Values should be expressed as\n",
    "    a percentage of the total variance explained.\n",
    "\n",
    "    The DataFrame will have one row and each column should correspond to a\n",
    "    principal component.\n",
    "\n",
    "    Example:\n",
    "             PC1        PC2        PC3        PC4    PC5\n",
    "    0  36.198848  55.406338  66.529969  73.598999  100.0\n",
    "\n",
    "    If scale is True, you should standardise the data first\n",
    "    Tip: use the StandardScaler from sklearn\n",
    "\n",
    "    :param df: pandas DataFrame\n",
    "    :param scale: boolean, whether to scale or not\n",
    "    :return: a new DataFrame with cumulated variance in percent\n",
    "    \"\"\"\n",
    "    # Create instance of pca\n",
    "    pca = PCA()\n",
    "\n",
    "    # scale if required scale then fit pca on scaled data\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(df)\n",
    "        pca.fit_transform(scaled_data)\n",
    "    # else fit\n",
    "    else:\n",
    "        pca.fit_transform(df)\n",
    "\n",
    "    # Calculate cumalative sum of variance\n",
    "    cum_var_sum = np.cumsum(100 * pca.explained_variance_ratio_)\n",
    "\n",
    "    # Create column names varaiable PC1 ... PCn\n",
    "    col_names = ['PC' + str(i+1) for i in range(df.columns.size)]\n",
    "\n",
    "    # Convert to a dataframe, transpose and set column names\n",
    "    cum_var_df = pd.DataFrame(cum_var_sum, index=col_names).T\n",
    "\n",
    "    return cum_var_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates_of_first_two(df, scale):\n",
    "    \"\"\"Apply PCA on a given DataFrame df and return a new DataFrame\n",
    "    containing the coordinates of the first two principal components\n",
    "    expressed in the original basis (with the original columns).\n",
    "\n",
    "    Example:\n",
    "    if the original DataFrame was:\n",
    "\n",
    "          A    B\n",
    "    0   1.3  1.2\n",
    "    1  27.0  2.1\n",
    "    2   3.3  6.8\n",
    "    3   5.1  3.2\n",
    "\n",
    "    we want the components PC1 and PC2 expressed as a linear combination\n",
    "    of A and B, presented in a table as:\n",
    "\n",
    "              A      B\n",
    "    PC1    0.99  -0.06\n",
    "    PC2    0.06   0.99\n",
    "\n",
    "    If scale is True, you should standardise the data first\n",
    "    Tip: use the StandardScaler from sklearn\n",
    "\n",
    "    :param df: pandas DataFrame\n",
    "    :param scale: boolean, whether to scale or not\n",
    "    :return: a new DataFrame with coordinates of PC1 and PC2\n",
    "    \"\"\"\n",
    "\n",
    "    # Create instance of pca\n",
    "    pca = PCA()\n",
    "\n",
    "    # scale if required scale then fit pca on scaled data\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(df)\n",
    "        pca.fit_transform(scaled_data)\n",
    "    # else fit\n",
    "    else:\n",
    "        pca.fit_transform(df)\n",
    "\n",
    "    # Get first two components\n",
    "    top_cmp = pca.components_[0:2]\n",
    "\n",
    "    # Convert to dataframe and set row index and col names\n",
    "    top_two = pd.DataFrame(\n",
    "        top_cmp, index=['PC1', 'PC2'], columns=df.columns.values)\n",
    "\n",
    "    return top_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_important_two(df, scale):\n",
    "    \"\"\"Apply PCA on a given DataFrame df and use it to determine the\n",
    "    'most important' features in your dataset. To do so we will focus\n",
    "    on the principal component that exhibits the highest explained\n",
    "    variance (that's PC1).\n",
    "\n",
    "    PC1 can be expressed as a vector with weight on each of the original\n",
    "    columns. Here we want to return the names of the two features that\n",
    "    have the highest weights in PC1 (in absolute value).\n",
    "\n",
    "    Example:\n",
    "        if the original DataFrame was:\n",
    "\n",
    "          A    B     C\n",
    "     0  1.3  1.2   0.1\n",
    "     1  2.0  2.1   1.2\n",
    "     2  3.3  6.8  23.4\n",
    "     3  5.1  3.2   4.5\n",
    "\n",
    "    and PC1 can be written as [0.05, 0.22, 0.97] in [A, B, C].\n",
    "\n",
    "    Then you should return C, B as the two most important features.\n",
    "\n",
    "    If scale is True, you should standardise the data first\n",
    "    Tip: use the StandardScaler from sklearn\n",
    "\n",
    "    :param df: pandas DataFrame\n",
    "    :param scale: boolean, whether to scale or not\n",
    "    :return: names of the two most important features as a tuple\n",
    "    \"\"\"\n",
    "\n",
    "    # call function to scale if required and fit model which returns\n",
    "    # PC1 and PC2 coordeinates in order to access that PC1 data\n",
    "    top_two = get_coordinates_of_first_two(df, scale)\n",
    "\n",
    "    #  Get  absolute value of coordinates of pc1\n",
    "    pc1 = np.abs(top_two.iloc[0])\n",
    "\n",
    "    # Retuen indices corresponding to the sort\n",
    "    # flip to get descending order and select the\n",
    "    # indices of the first 2 (max) values\n",
    "    max_indices = np.flip(np.argsort(pc1))[0:2]\n",
    "\n",
    "    # extract columsn relating to indices return as a tuple\n",
    "    return tuple(df.columns[max_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_in_n_dimensions(df, point_a, point_b, n, scale):\n",
    "    \"\"\"Write a function that applies PCA on a given DataFrame df in order\n",
    "    to find a new subspace of dimension n.\n",
    "\n",
    "    Transform the two points point_a and point_b to be represented into that\n",
    "    n dimensions space, compute the Euclidean distance between the points in\n",
    "    that space and return it.\n",
    "\n",
    "    Example:\n",
    "        if the original DataFrame was:\n",
    "\n",
    "          A    B     C\n",
    "     0  1.3  1.2   0.1\n",
    "     1  2.0  2.1   1.2\n",
    "     2  3.3  6.8  23.4\n",
    "     3  5.1  3.2   4.5\n",
    "\n",
    "    and n = 2, you can learn a new subspace with two columns [PC1, PC2].\n",
    "\n",
    "    Then given two points:\n",
    "\n",
    "    point_a = [1, 2, 3]\n",
    "    point_b = [2, 3, 4]\n",
    "    expressed in [A, B, C]\n",
    "\n",
    "    Transform them to be expressed in [PC1, PC2], here we would have:\n",
    "    point_a -> [-4.57, -1.74]\n",
    "    point_b -> [-3.33, -0.65]\n",
    "\n",
    "    and return the Euclidean distance between the points\n",
    "    in that space.\n",
    "\n",
    "    If scale is True, you should standardise the data first\n",
    "    Tip: use the StandardScaler from sklearn\n",
    "\n",
    "    :param df: pandas DataFrame\n",
    "    :param point_a: a numpy vector expressed in the same basis as df\n",
    "    :param point_b: a numpy vector expressed in the same basis as df\n",
    "    :param n: number of dimensions of the new space\n",
    "    :param scale: whether to scale data or not\n",
    "    :return: distance between points in the subspace\n",
    "    \"\"\"\n",
    "\n",
    "    # call function to scale if required and fit model\n",
    "    # Create instance of pca\n",
    "    pca = PCA(n)\n",
    "\n",
    "    # scale if required scale then fit pca on scaled data\n",
    "    if scale:\n",
    "        scaler = StandardScaler()\n",
    "        transformed_data = scaler.fit_transform(df)\n",
    "        # converts to a 2d array and then transforms\n",
    "        transformed_point_a = scaler.transform(point_a.reshape(1, -1))\n",
    "        transformed_point_b = scaler.transform(point_b.reshape(1, -1))\n",
    "    # if scale is false then set to use the original data\n",
    "    else:\n",
    "        transformed_data = df\n",
    "        transformed_point_a = point_a.reshape(1, -1)\n",
    "        transformed_point_b = point_b.reshape(1, -1)\n",
    "\n",
    "    # Apply PCA\n",
    "    pca.fit_transform(transformed_data)\n",
    "    # Transform points a and b\n",
    "    a_transformed = pca.transform(transformed_point_a)\n",
    "    b_transformed = pca.transform(transformed_point_b)\n",
    "\n",
    "    # Calculate the L2 norm\n",
    "    distance = np.linalg.norm(a_transformed - b_transformed, 2)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
